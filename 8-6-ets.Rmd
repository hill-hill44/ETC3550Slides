---
title: "8. Exponential smoothing"
author: "8.6 Estimation and model selection"
date: "OTexts.org/fpp3/"
classoption: aspectratio=169
titlepage: fpp3title.png
titlecolor: fpp3red
toc: false
output:
  binb::monash:
    colortheme: monashwhite
    fig_width: 7.5
    fig_height: 3
    keep_tex: no
    includes:
      in_header: fpp3header.tex
---

```{r setup, include=FALSE}
source("setup.R")
```

## Estimating ETS models

  * Smoothing parameters $\alpha$, $\beta$, $\gamma$ and $\phi$, and the initial states $\ell_0$, $b_0$, $s_0,s_{-1},\dots,s_{-m+1}$ are estimated by maximising the "likelihood" = the probability of the data arising from the specified model.
  * For models with additive errors equivalent to minimising SSE.
  * For models with multiplicative errors, \textbf{not} equivalent to minimising SSE.

## Innovations state space models

Let $\bm{x}_t = (\ell_t, b_t, s_t, s_{t-1}, \dots, s_{t-m+1})$ and
$\varepsilon_t\stackrel{\mbox{\scriptsize iid}}{\sim}
\mbox{N}(0,\sigma^2)$.
\begin{block}{}
\begin{tabular}{lcl}
$y_t$ &=& $\underbrace{h(\bm{x}_{t-1})} +
\underbrace{k(\bm{x}_{t-1})\varepsilon_t}$\\
&& \hspace*{0.5cm}$\mu_t$ \hspace*{1.45cm} $e_t$ \\[0.2cm]
$\bm{x}_t$ &=& $f(\bm{x}_{t-1}) +
g(\bm{x}_{t-1})\varepsilon_t$\\
\end{tabular}
\end{block}\fontsize{14}{16}\sf

Additive errors
: \mbox{}\vspace*{-0.cm}\newline
  $k(x)=1$.\qquad $y_t = \mu_{t} + \varepsilon_t$.

Multiplicative errors
: \mbox{}\vspace*{-0.cm}\newline
  $k(\bm{x}_{t-1}) = \mu_{t}$.\qquad $y_t = \mu_{t}(1 + \varepsilon_t)$.\newline
  $\varepsilon_t = (y_t - \mu_t)/\mu_t$ is relative error.

## Innovations state space models

\alert{Estimation}\vspace*{0.5cm}

\begin{block}{}\vspace*{-0.7cm}
\begin{align*}
L^*(\bm\theta,\bm{x}_0) &= T\log\!\bigg(\sum_{t=1}^T \varepsilon^2_t\!\bigg) + 2\sum_{t=1}^T \log|k(\bm{x}_{t-1})|\\
&= -2\log(\text{Likelihood}) + \mbox{constant}
\end{align*}
\end{block}

* Estimate parameters $\bm\theta = (\alpha,\beta,\gamma,\phi)$ and
initial states $\bm{x}_0 = (\ell_0,b_0,s_0,s_{-1},\dots,s_{-m+1})$ by
minimizing $L^*$.

## Parameter restrictions
\fontsize{13}{14}\sf

### *Usual* region

  * Traditional restrictions in the methods $0< \alpha,\beta^*,\gamma^*,\phi<1$\newline (equations interpreted as weighted averages).
  * In models we set $\beta=\alpha\beta^*$ and $\gamma=(1-\alpha)\gamma^*$.
  * Therefore $0< \alpha <1$, &nbsp;&nbsp; $0 < \beta < \alpha$ &nbsp;&nbsp; and $0< \gamma < 1-\alpha$.
  * $0.8<\phi<0.98$ --- to prevent numerical difficulties.
 \pause

### *Admissible* region

  * To prevent observations in the distant past having a continuing effect on current forecasts.
  * Usually (but not always) less restrictive than \textit{traditional} region.
  * For example for ETS(A,N,N): \newline \textit{traditional} $0< \alpha <1$ while \textit{admissible} $0< \alpha <2$.

## Model selection
\fontsize{13}{14}\sf

\begin{block}{Akaike's Information Criterion}
\[
\text{AIC} = -2\log(\text{L}) + 2k
\]
\end{block}\vspace*{-0.2cm}
where $L$ is the likelihood and $k$ is the number of parameters & initial states estimated in the model.\pause

\begin{block}{Corrected AIC}
\[
\text{AIC}_{\text{c}} = \text{AIC} + \frac{2k(k+1)}{T-k-1}
\]
\end{block}
which is the AIC corrected (for small sample bias).
\pause
\begin{block}{Bayesian Information Criterion}
\[
\text{BIC} = \text{AIC} + k[\log(T)-2].
\]
\end{block}

## AIC and cross-validation

\Large

\begin{alertblock}{}
Minimizing the AIC assuming Gaussian residuals is asymptotically equivalent to minimizing one-step time series cross validation MSE.
\end{alertblock}


## Automatic forecasting

**From Hyndman et al.\ (IJF, 2002):**

* Apply each model that is appropriate to the data.
Optimize parameters and initial values using MLE (or some other
criterion).
* Select best method using AICc:
* Produce forecasts using best method.
* Obtain forecast intervals using underlying state space model.

Method performed very well in M3 competition.

## Some unstable models

* Some of the combinations of (Error, Trend, Seasonal) can lead to numerical difficulties; see equations with division by a state.
* These are: ETS(A,N,M), ETS(A,A,M), ETS(A,A\damped,M).
* Models with multiplicative errors are useful for strictly positive data, but are not numerically stable with data containing zeros or negative values. In that case only the six fully additive models will be applied.

## Example: National populations
\fontsize{9}{9}\sf

```{r popfit, echo=TRUE, cache=TRUE}
fit <- global_economy |>
  mutate(Pop = Population / 1e6) |>
  model(ets = ETS(Pop))
fit
```

## Example: National populations
\fontsize{9}{9}\sf

```{r popfc, echo=TRUE, cache=TRUE, dependson="popfit"}
fit |>
  forecast(h = 5)
```

## Example: Australian holiday tourism
\fontsize{9}{9}\sf

```{r ausholidays-fit, echo=TRUE}
holidays <- tourism |>
  filter(Purpose == "Holiday")
fit <- holidays |> model(ets = ETS(Trips))
fit
```

## Example: Australian holiday tourism
\fontsize{9}{9}\sf

```{r ausholidays-report}
fit |>
  filter(Region == "Snowy Mountains") |>
  report()
```

## Example: Australian holiday tourism
\fontsize{9}{9}\sf

```{r ausholidays-components}
fit |>
  filter(Region == "Snowy Mountains") |>
  components(fit)
```

## Example: Australian holiday tourism

```{r ausholidays-components-plot}
fit |>
  filter(Region == "Snowy Mountains") |>
  components(fit) |>
  autoplot()
```

## Example: Australian holiday tourism
\fontsize{9}{9}\sf

```{r ausholidays-forecast}
fit |> forecast()
```

## Example: Australian holiday tourism

```{r ausholidays-forecast-plot, fig.height=2.6}
fit |>
  forecast() |>
  filter(Region == "Snowy Mountains") |>
  autoplot(holidays) +
  labs(y = "Thousands", title = "Overnight trips")
```

## Residuals

### Response residuals
$$\hat{e}_t = y_t - \hat{y}_{t|t-1}$$

### Innovation residuals
Additive error model:
$$\hat\varepsilon_t = y_t - \hat{y}_{t|t-1}$$

Multiplicative error model:
$$\hat\varepsilon_t = \frac{y_t - \hat{y}_{t|t-1}}{\hat{y}_{t|t-1}}$$

## Example: Australian holiday tourism
\fontsize{8}{7.5}\sf

```{r, echo = TRUE}
aus_holidays <- tourism |>
  filter(Purpose == "Holiday") |>
  summarise(Trips = sum(Trips))
fit <- aus_holidays |>
  model(ets = ETS(Trips)) |>
  report()
```

## Example: Australian holiday tourism
\fontsize{10}{10}\sf

```{r, echo = TRUE, results = "hide"}
residuals(fit)
residuals(fit, type = "response")
```

```{r, echo=FALSE}
bind_rows(
  residuals(fit) |> mutate(Type = "Innovation residuals") |> as_tibble(),
  residuals(fit, type = "response") |> mutate(Type = "Response residuals") |> as_tibble()
) |>
  ggplot(aes(x = Quarter, y = .resid)) +
  geom_line() +
  facet_grid(Type ~ ., scales = "free_y") +
  labs(y = "")
```

## Example: Australian holiday tourism
\fontsize{10}{9.5}\sf

```{r wider, include=FALSE}
fred <- options(width = 67)
```

```{r tourismresiduals, dependson="wider"}
fit |>
  augment()
```

\only<2>{\begin{textblock}{8}(7,2)\begin{alertblock}{}Innovation residuals (\texttt{.innov}) are given by $\hat{\varepsilon}_t$ while regular residuals (\texttt{.resid}) are $y_t - \hat{y}_{t-1}$. They are different when the model has multiplicative errors.
\end{alertblock}\end{textblock}}

```{r normal, include=FALSE}
options(fred)
```
